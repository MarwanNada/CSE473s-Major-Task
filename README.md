Course: CSE473s: Computational Intelligence (Fall 2025)

University: Ain Shams University, Faculty of Engineering

Project Overview

This project implements a modular Deep Learning library from scratch using only Python and NumPy. The goal is to demystify the internal mechanics of neural networks by implementing forward propagation, backpropagation (Gradient Descent), and various activation/loss functions manually.

Features

Modular Design: Layers, Activations, Losses, and Optimizers are separate classes.

Layers: Fully Connected (Dense) layers.

Activations: Sigmoid, Tanh, ReLU, Softmax.

Losses: Mean Squared Error (MSE).

Optimization: Stochastic Gradient Descent (SGD).


To run the XOR demo and Gradient Checking:

Navigate to the notebooks folder.

Open project_demo.ipynb.

Run all cells to see the library in action.

Results:

Gradient Checking: Validated numerical vs. analytical gradients with < 1e-7 error.

XOR Problem: Successfully solved the XOR non-linear classification task using a 2-4-1 architecture.